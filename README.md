
# Stack Docker: Airflow + MySQL + Hadoop (HDFS)

> **Status do pacote**: README gerado automaticamente em 2025-09-24 a partir do ZIP enviado. Foram detectados alguns pontos a ajustar (veja **Checklist de corre√ß√µes**).

## üéØ Objetivo
Provisionar um ambiente local com:
- **MySQL** (banco de dados)
- **Hadoop HDFS** (armazenamento distribu√≠do)
- **Airflow** (orquestra√ß√£o de DAGs)
- DAGs Python que l√™em do MySQL, transformam e salvam sa√≠das no HDFS.

## üì¶ Estrutura detectada
```
‚îú‚îÄ dag1.py
‚îú‚îÄ dag2.py
‚îú‚îÄ dag3.py
‚îú‚îÄ data.sql
‚îú‚îÄ docker-compose.yml
‚îú‚îÄ requiriments.txt
‚îú‚îÄ shells.md
```

## üîé Observa√ß√µes sobre o `docker-compose.yml`
O arquivo cont√©m o seguinte in√≠cio (trecho relevante):

```yaml
version: '3.8'

services:
  mysql:
    image: mysql:latest
    environment:
      MYSQL_ROOT_PASSWORD: fiapOn
      MYSQL_DATABASE: dbfiapon
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql

  hadoop:
    image: bde2020/hadoop-python:latest
    ports:
      - "9870:9870"
    volumes:
      - hadoop_data:/hadoop/dfs

  airflow:
    image: apache/airflow:2.5.0
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+pymysql://root:fiapOn@mysql/dbfiapon
      AIRFLOW__WEBSERVER__RBAC: "true"
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/usr/local/airflow/dags
      - ./requirements.txt:/requirements.txt
    depends_on:
      - mysql
    command: >
      bash -c "pip install -r /requirements.txt &&
               airflow db init &&
               airflow scheduler & airflow webserver"

volumes:
  mysql_data:
  hadoop_data:
```

> ‚ö†Ô∏è Notamos que h√° `...` no meio do arquivo ‚Äî indicando trechos faltando. Abaixo h√° um **compose de refer√™ncia** funcional para voc√™ comparar e completar.

## ‚úÖ Checklist de corre√ß√µes recomendadas
- [ ] Arquivo `requiriments.txt` encontrado; o compose referencia `requirements.txt`. Renomear e corrigir conte√∫do.
- [ ] Pasta `dags/` n√£o existe, mas o compose monta `./dags` no cont√™iner do Airflow.
- [ ] `dag1.py` usa senha MySQL sem aspas (`password=fiapOn`). Corrigir para `password='fiapOn'` ou usar vari√°vel de ambiente.
- [ ] `dag2.py` usa senha MySQL sem aspas (`password=fiapOn`). Corrigir para `password='fiapOn'` ou usar vari√°vel de ambiente.
- [ ] `dag3.py` usa senha MySQL sem aspas (`password=fiapOn`). Corrigir para `password='fiapOn'` ou usar vari√°vel de ambiente.
- [ ] `shells.md` est√° vazio (opcional remover ou preencher).


### Itens espec√≠ficos
1. **Criar a pasta `dags/`** e **mover** `dag1.py`, `dag2.py`, `dag3.py` para dentro dela.
2. **Renomear** `requiriments.txt` ‚Üí `requirements.txt` e garantir conte√∫do m√≠nimo:
   ```txt
   apache-airflow==2.10.*
   pymysql
   hdfs
   ```
3. **Corrigir credenciais** em DAGs:
   ```python
   pymysql.connect(
       host='mysql',
       user='root',
       password='fiapOn',  # ou vari√°vel de ambiente
       database='dbfiapon',
   )
   ```
4. **Popular MySQL automaticamente (opcional):** coloque `data.sql` em `./mysql-init/` e monte em `/docker-entrypoint-initdb.d/` no servi√ßo MySQL.

## üß© Compose de refer√™ncia (exemplo funcional)
Use-o para **comparar** com o seu. **Ajuste portas/vers√µes conforme necessidade**.

```yaml
version: '3.8'

services:
  mysql:
    image: mysql:8.0
    container_name: mysql
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD:-fiapOn}
      MYSQL_DATABASE: ${MYSQL_DATABASE:-dbfiapon}
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./mysql-init:/docker-entrypoint-initdb.d:ro

  hadoop:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: hadoop
    environment:
      CLUSTER_NAME: local-hadoop
    ports:
      - "9870:9870"   # Web UI NameNode
      - "9000:9000"   # RPC NameNode
    volumes:
      - hadoop_data:/hadoop/dfs/name

  airflow:
    image: apache/airflow:2.10.2
    container_name: airflow
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__WEBSERVER__SECRET_KEY: 'change-me'
      _PIP_ADDITIONAL_REQUIREMENTS: ""
    user: "0:0"
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./requirements.txt:/requirements.txt:ro
      - airflow_logs:/opt/airflow/logs
    depends_on:
      - mysql
    command: >
      bash -c "pip install -r /requirements.txt &&
               airflow db init &&
               airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin &&
               airflow scheduler & exec airflow webserver"

volumes:
  mysql_data:
  hadoop_data:
  airflow_logs:
```

> **Dica:** se preferir um √∫nico cont√™iner Hadoop all-in-one (para testes), voc√™ pode usar `bde2020/hadoop:latest` e expor `9870`.

## üêç Exemplo m√≠nimo de DAG (corrigido)
Coloque em `dags/dag1.py`:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import pymysql
from hdfs import InsecureClient

def transform_and_save_to_hadoop():
    conn = pymysql.connect(
        host='mysql',
        user='root',
        password='fiapOn',
        database='dbfiapon',
    )
    try:
        with conn.cursor() as cur:
            cur.execute("SELECT id, nome FROM tabela1")
            data = cur.fetchall()
        transformed = [f"{row[0]}, {row[1]}" for row in data]

        hdfs = InsecureClient('http://hadoop:9870', user='hadoop')
        with hdfs.write('/user/hadoop/tabela1_output.txt', encoding='utf-8') as w:
            for line in transformed:
                w.write(line + "\\n")
    finally:
        conn.close()

with DAG(
    dag_id='dag1',
    start_date=datetime(2023, 1, 1),
    schedule_interval='@daily',
    catchup=False,
) as dag:
    task = PythonOperator(
        task_id='transform_and_save_to_hadoop',
        python_callable=transform_and_save_to_hadoop,
    )
```

## üöÄ Como subir o ambiente
1. **Pr√©-requisitos:** Docker e Docker Compose v2.
2. (Opcional) Crie um `.env` na raiz:
   ```env
   MYSQL_ROOT_PASSWORD=fiapOn
   MYSQL_DATABASE=dbfiapon
   ```
3. Suba os servi√ßos:
   ```bash
   docker compose up -d
   ```
4. Acesse:
   - Airflow: http://localhost:8080 (usu√°rio/senha conforme criado no comando)
   - MySQL: `localhost:3306` (root / `fiapOn` por padr√£o)
   - HDFS NameNode UI: http://localhost:9870

## üß™ Testes r√°pidos
- Verifique Airflow **DAGs** aparecendo na UI.
- No MySQL, crie `tabela1` e insira registros (ou use `data.sql` via `mysql-init/`).
- Rode a DAG e confirme `tabela1_output.txt` no HDFS.

## üîê Boas pr√°ticas
- Nunca commitar senhas reais. Use `.env` ou `secrets`.
- Fixar vers√µes das imagens para reprodutibilidade.
- Parametrizar conex√µes no Airflow (Connections/Variables) ao inv√©s de strings fixas no c√≥digo.

## üõ†Ô∏è Troubleshooting
- **Airflow n√£o sobe / d√° erro no `requirements.txt`:** confira nome correto, conte√∫do e permiss√µes de montagem.
- **DAG n√£o aparece:** verifique se est√° em `dags/` e se n√£o h√° erros de sintaxe (veja logs do scheduler).
- **HDFS write falha:** valide se a porta `9870` est√° exposta e o path `/user/hadoop/` existe.
- **MySQL init n√£o rodou:** confirme a pasta `./mysql-init/` e logs do cont√™iner MySQL.

---

Feito com üíö para acelerar seu ambiente de dados.
